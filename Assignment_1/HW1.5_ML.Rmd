---
title: "HW1.5_ML"
author: "Matthew Lucich"
date: "9/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r }
library(dplyr)
library(Metrics)
library(MLmetrics)
library(leaps)
library(car)
```

## Load Data

```{r echo=FALSE}
# load data
data <- read.csv('moneyball-training-data.csv')
```


### Create train and test data

```{r}
data$id <- 1:nrow(data)
train <- data %>% dplyr::sample_frac(.75)
test  <- dplyr::anti_join(data, train, by = 'INDEX')
```



```{r}

colSums(is.na(data))

data_clean <- na.omit(data) 

colSums(is.na(data_clean))

```


```{r}

train_clean <- data_clean %>% dplyr::sample_frac(.75)
test_clean  <- dplyr::anti_join(data_clean, train, by = 'INDEX')

colSums(is.na(train_clean))

```



## Model 3: Backward Elimination

```{r}

# Backward Elimination
backward_model <- lm(TARGET_WINS ~ TEAM_BASERUN_SB + TEAM_BATTING_HR
                 + TEAM_BATTING_BB + TEAM_BASERUN_SB
                 + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP
                 , data = train_clean)
summary(backward_model)

```


```{r}

# Make predictions on test set
backward_model_predictions = predict(backward_model, test_clean)

# Obtain RMSE between actuals and predicted
rmse(test_clean$TARGET_WINS, backward_model_predictions)

```


## Model 4: Forward Selection

```{r}

# Fit model
foward_model <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_FIELDING_E + TEAM_FIELDING_DP
                   + TEAM_PITCHING_HR + TEAM_PITCHING_SO, data = train_clean)
# View summary
summary(foward_model)

```


```{r}

# Make predictions on test set
forward_model_predictions = predict(foward_model, test_clean)

# Obtain RMSE between actuals and predicted
rmse(test_clean$TARGET_WINS, forward_model_predictions)

```


## Validating OLS Assumptions

```{r}
# Assumption: No Multicollinearity (VIF under 5)
vif(foward_model)
```

```{r}
# Assumption: Mean of residuals is zero
mean(residuals(foward_model))
```


```{r}
# Assumption: Homoscedasticity of residuals
plot(foward_model)
```


```{r}
# Assumption: No auto-correlation
acf(residuals(foward_model), lags=20)
```

## Model Selection

The two primary statistics used to choose our final model were adjusted r-squared and root mean square error (RMSE). 

Adjusted r-squared helped guide model selection since, like r-squared, adjusted r-squared measures the amount of variation in the dependent variable explained by the independent variables, except with a correction to ensure only independent variables with predictive power raise the statistic.

RMSE was perhaps even more crucial to model selection as it is the measure of the standard deviation of the residuals.

However, before evaluating those statistics we first validated that all our individual predictors had p-values below 0.05, the cutoff for a 95% confidence interval. Additionally, we validated that the F-statistic was also significant at a 95% confidence interval.


## References

Bhandari, Aniruddha, Analytics Vidhya, 2020 https://www.analyticsvidhya.com/blog/2020/07/difference-between-r-squared-and-adjusted-r-squared/ 

