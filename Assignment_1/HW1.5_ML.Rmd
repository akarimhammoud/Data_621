---
title: "HW1 ML Take #2"
author: "Matthew Lucich"
date: "9/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```


```{r }
library(dplyr)
library(Metrics)
library(MLmetrics)
library(leaps)
library(car)
library(MASS)

library(tidyverse)
library(caret)
library(leaps)
```


### Load the Data
```{r}
train_raw <-read.csv("https://raw.githubusercontent.com/akarimhammoud/Data_621/main/Assignment_1/data/moneyball-training-data.csv")

evaluation_raw <-read.csv("https://raw.githubusercontent.com/akarimhammoud/Data_621/main/Assignment_1/data/moneyball-evaluation-data.csv")
```



# Variable Distributions

```{r}
train_raw %>%
  gather(variable, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "#3A8B63", color="#3A8B63") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())
```

# Log Variable Distributions

```{r}

train_log <- log(train_raw)

train_log %>%
  gather(variable, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "#3A8B63", color="#3A8B63") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())

```

# Add log transformed variables to training set, for the variables that appear more normal when log transformed

```{r}

train_raw$TEAM_BASERUN_CS_log <- log(train_raw$TEAM_BASERUN_CS)
train_raw$TEAM_BASERUN_SB_log <- log(train_raw$TEAM_BASERUN_SB)
train_raw$TEAM_BATTING_3B_log <- log(train_raw$TEAM_BATTING_3B)
train_raw$TEAM_BATTING_H_log <- log(train_raw$TEAM_BATTING_H)
train_raw$TEAM_PITCHING_BB_log <- log(train_raw$TEAM_PITCHING_BB)

```



## Remove NAs

```{r}

# Review NA counts
colSums(is.na(train_raw))

# Remove NAs
train_no_na <- na.omit(train_raw) 
evaluation_no_na <- na.omit(evaluation_raw) 

# Confirm NAs were removed
colSums(is.na(train_no_na))

colSums(is.na(evaluation_no_na))
```


## Create Train, Test Split

```{r}

train_all <- train_no_na %>% dplyr::sample_frac(.75)
test  <- dplyr::anti_join(train_no_na, train_all, by = 'INDEX')

```


## Exclude variables that do not meet normality assumption

```{r}

train <- train_all %>% dplyr::select(TARGET_WINS, TEAM_BATTING_2B, TEAM_BATTING_BB, 
                                             TEAM_FIELDING_DP, TEAM_BASERUN_CS_log, TEAM_BASERUN_SB_log, 
                                             TEAM_BATTING_3B_log, TEAM_BATTING_H_log, TEAM_PITCHING_BB_log)

```


## Automated AIC

```{r}
# Source: http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/

full.model <- lm(TARGET_WINS ~., data = train)
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both", 
                      trace = FALSE)
summary(step.model)

```


## Automated leapBackward

```{r}

# Set seed for reproducibility
set.seed(212)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
leapBackward <- train(TARGET_WINS ~., data = train,
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:8),
                    trControl = train.control
                    )
leapBackward$results

```



## Automated leapForward

```{r}

# Set seed for reproducibility
set.seed(212)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
leapForward.model <- train(TARGET_WINS ~., data = train,
                    method = "leapForward", 
                    tuneGrid = data.frame(nvmax = 1:8),
                    trControl = train.control
                    )
leapForward.model$results

```



## Automated leapSeq

```{r}

# Set seed for reproducibility
set.seed(212)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
leapSeq.model <- train(TARGET_WINS ~., data = train,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax = 1:8),
                    trControl = train.control
                    )
leapSeq.model$results

```


# Best model out of the automated: leapForward, leapBackward, leapSeq

```{r}

summary(leapForward.model$finalModel)

```


## Automated model Leap Forward

```{r}

# Fit model
automated_forward_model <- lm(formula = TARGET_WINS ~ TEAM_BATTING_2B + TEAM_BATTING_BB + TEAM_FIELDING_DP 
                              + TEAM_BASERUN_CS_log + TEAM_BASERUN_SB_log + TEAM_BATTING_3B_log 
                              + TEAM_BATTING_H_log + TEAM_PITCHING_BB_log, data = train)
# View summary
summary(automated_forward_model)

```


## Automated model Step AIC

```{r}

# Fit model
automated_aic_model <- lm(formula = TARGET_WINS ~ TEAM_BATTING_BB + TEAM_FIELDING_DP + TEAM_BATTING_H_log
                      , data = train)
# View summary
summary(automated_aic_model)

```


```{r}

# Make predictions on test set
automated_model_predictions = predict(automated_aic_model, test)

# Obtain RMSE between actuals and predicted
rmse(test$TARGET_WINS, automated_model_predictions)

```


# ORIGINAL MODELS


## Model 3: Backward Elimination Model

```{r}

# Fit model
backward_model <- lm(TARGET_WINS ~ TEAM_BASERUN_SB + TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_BASERUN_SB 
                     + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP, data = train_all)
# View summary
summary(backward_model)

```


```{r}

# Make predictions on test set
backward_model_predictions = predict(backward_model, test)

# Obtain RMSE between actuals and predicted
rmse(test$TARGET_WINS, backward_model_predictions)

```


## Model 4: Forward Selection Model

```{r}

# Fit model
foward_model <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_FIELDING_E + TEAM_FIELDING_DP
                   + TEAM_PITCHING_HR + TEAM_PITCHING_SO, data = train_all)
# View summary
summary(foward_model)

```


```{r}

# Make predictions on test set
forward_model_predictions = predict(foward_model, test)

# Obtain RMSE between actuals and predicted
rmse(test$TARGET_WINS, forward_model_predictions)

```


## Verifying OLS Regression Assumptions

```{r}
# Assumption: No Multicollinearity (VIF under 5)
vif(foward_model)
```

```{r}
# Assumption: Mean of residuals is zero
mean(residuals(foward_model))
```


```{r}
# Assumption: Homoscedasticity of residuals
plot(foward_model)
```


```{r}
# Assumption: No auto-correlation
acf(residuals(foward_model), lags=20)
```

## Model Selection

First, before fully evaluating models we validated that all individual predictors had p-values below 0.05, the cutoff for a 95% confidence level. Additionally, we validated that the models F-statistics were also significant at a 95% confidence level.

Then, the two primary statistics used to choose our final model were adjusted R-squared and root mean square error (RMSE). Adjusted R-squared helped guide model selection since, like R-squared, adjusted R-squared measures the amount of variation in the dependent variable explained by the independent variables, except with a correction to ensure only independent variables with predictive power raise the statistic. RMSE was perhaps even more crucial to model selection as it is the measure of the standard deviation of the residuals, essentially a measure of accuracy in the same units as the response variable. To ensure the model can generalize to unobserved data, we calculated the RMSE on our test set. 

Both of our top models–forward selection and backward elimination–saw a RMSE of approximately 9. Therefore, we chose the forward selection model due to its slightly higher adjusted R-squared. Additionally, since both top performing models included six predictors, parsimony was not a consideration.

Lastly, we verified the forward selection model meets OLS regression assumptions. These included: no significant multicollinearity, the mean of residuals is zero, homoscedasticity of residuals, and no significant auto-correlation. We deemed all assumptions had been met, but note, there is a slight trend in the residuals vs fitted plot (Assumption: Homoscedasticity of residuals) which may indicate a small nonlinear trend.




## Matt's References

Bhandari, Aniruddha, "Key Difference between R-squared and Adjusted R-squared for Regression Analysis", Analytics Vidhya, 2020 https://www.analyticsvidhya.com/blog/2020/07/difference-between-r-squared-and-adjusted-r-squared/ 

Glen., Stephanie "RMSE: Root Mean Square Error", StatisticsHowTo.com https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/

Gupta, Aryansh, "Linear Regression Assumptions and Diagnostics in R", RPubs, https://rpubs.com/aryn999/LinearRegressionAssumptionsAndDiagnosticsInR 

Kim, Bommae, "Understanding Diagnostic Plots for Linear Regression Analysis", University of Virginia Library, https://data.library.virginia.edu/diagnostic-plots/ 




