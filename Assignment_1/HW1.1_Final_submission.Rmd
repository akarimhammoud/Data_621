---
title: "Data 621 Assignment 1"
subtitle: ""
author: "Group2 - Gabriella Martinez, Maliat Islam, Ken Popkin, George Cruz Deschamps, Matthew Lucich and Karim Hammoud"
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: pygments
    theme: cerulean 
    toc: yes
    toc_float: yes
  pdf_document: default
  openintro::lab_report: default
editor_options:
  chunk_output_type: console
---


![](https://raw.githubusercontent.com/akarimhammoud/Data_621/main/Assignment_1/data/1.png)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

In this data set we are trying to identify good and bad teams in major league baseball teamâ€™s season. We are assuming some of the predictors will be higher for good teams. We will try to predict how many times a team will win in this season.

```{r, include = FALSE}
library(tidyverse)
library(corrplot)
library(pacman)
library(here)
library(RCurl)
library(naniar)
library(UpSetR)
library(mice)
library(Metrics)
library(MLmetrics)
library(caret)
library(leaps)
library(MASS)
library(dplyr)
```


## DATA EXPLORATION:
We can observe the response variable (TARGET_WINS) looks to be normally distributed. This supports the working theory that there are good teams and bad teams. There are also a lot of average teams.

There are also quite a few variables with missing values. and,Some variables are right skewed (TEAM_BASERUN_CS, TEAM_BASERUN_SB, etc.). This might support the good team theory. It may also introduce non-normally distributed residuals in the model. We shall see.

### Load the Data

```{r, echo=FALSE, results='asis', cache=TRUE, include=FALSE}
# Set seed for reproducibility
set.seed(621)

train <-read.csv("https://raw.githubusercontent.com/akarimhammoud/Data_621/main/Assignment_1/data/moneyball-training-data.csv")

evaluation <-read.csv("https://raw.githubusercontent.com/akarimhammoud/Data_621/main/Assignment_1/data/moneyball-evaluation-data.csv")
```

Summary of the train data
```{r echo=FALSE}
summary(train)

```

Summary of the test data
```{r echo=FALSE}
summary(evaluation)
```

Glimpse of the train data
```{r, echo=FALSE}
glimpse(train)

```

Glimpse of the test data
```{r, echo=FALSE}
glimpse(evaluation)
```

Find SD for all of the train data
```{r,echo=FALSE}

apply(evaluation,2,sd, na.rm=TRUE)
```

Find SD for all of the test data
```{r,echo=FALSE}
apply(train,2,sd, na.rm=TRUE)

```

Box plot the train data 
```{r, echo=FALSE}
ggplot(stack(train), aes(x = ind, y = values)) + 
  geom_boxplot() +
  theme(legend.position="none") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) 
```

Variable Distributions

```{r, echo=FALSE}
train %>%
  gather(variable, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "#3A8B63", color="#3A8B63") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())
```

Log Variable Distributions

```{r, echo=FALSE}
train_log <- log(train)

train_log %>%
  gather(variable, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "#3A8B63", color="#3A8B63") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())

```

Correlations with Response Variable
```{r warning=FALSE, echo=FALSE}
train %>%
  gather(variable, value, -TARGET_WINS) %>%
  ggplot(., aes(value, TARGET_WINS)) + 
  geom_point(fill = "#628B3A", color="#628B3A") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")
```

```{r, echo=FALSE}
train %>% 
  cor(., use = "complete.obs") %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", diag = FALSE)
```


### DATA PREPARATION
NA counts for the train data set
^[https://statisticsglobe.com/count-number-of-na-values-in-vector-and-column-in-r]
```{r, echo=FALSE}
#NA counts for the train data set
colSums(is.na(train))
```

visulaization and percentage of NA values
^[https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html]
```{r, echo=FALSE}
#visulaization and percentage of NA values
vis_miss(train)
```

alternative NA values visualization
^[https://datavizpyr.com/visualizing-missing-data-with-barplot-in-r/]
```{r, echo=FALSE}
#alternative NA values visualization
train  %>%
  summarise_all(list(~is.na(.)))%>%
  pivot_longer(everything(),
               names_to = "variables", values_to="missing") %>%
  count(variables, missing) %>%
  ggplot(aes(y=variables,x=n,fill=missing))+
  geom_col()
```

Since 92% of the data for the TEAM_BATTING_HBP is missing, the variable has been removed from both test and train data. TEAM_BASERUN_CS is a runner up with the next highest amount of NA at 34%.

```{r, echo=FALSE}
#removes the TEAM_BATTING_HBP due to high # of NAs
train_full <- train %>% dplyr::select(-c(TEAM_BATTING_HBP))
evaluation <- evaluation %>% dplyr::select(-c(TEAM_BATTING_HBP))
```

^[https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R-Manual/R-Manual5.html]
```{r, echo=FALSE}
#creates CSV in your current working directory of R
write.csv(train_full,'hw1_train_data.csv')
write.csv(evaluation, 'hw1_evaluation_data.csv')
```


```{r, echo=FALSE}
# Create train, test split
train <- train_full %>% dplyr::sample_frac(.75)
test  <- dplyr::anti_join(train_full, train, by = 'INDEX')
```


## BUILD MODELS


## Model #1
### Two predictors: Base hits by batters and Hits allowed
Using a manual review, below are the features selected for the first model and the supporting reason/s.

TEAM_BATTING_H = Base hits by batters:  it's impossible to win in baseball without getting to the bases and hitting the ball is the primary means to accomplish this.

TEAM_PITCHING_H = Hits allowed: winning without a good defense is difficult and in baseball preventing the other team from getting hits is a good defense strategy.

Only two features are selected for the first model - start small and build up seems like a good approach.

<B> Create the Regression Model </B>  
```{r, echo=FALSE}

# Build the first model and produce a summary
first_model <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_PITCHING_H, data = train)
summary(first_model)

```

The p values are 0, which per the criteria of "keep a feature if the p-value is <0.05" recommends that we keep both these features.  But, the adjusted R-squared is TERRIBLE at around 21%.  Even though the R-squared is poor it's simple to run this model with the test data, so we'll do that next. 

Evaluate the second model results using RMSE
```{r, echo=FALSE}

#Predict with the first model training data
first_model_predictions = predict(first_model, test)

#Evaluate the first model results using RMSE
rmse(test$TARGET_WINS, first_model_predictions)

```


## Model #2
### Four predictors: Base hits by batters, Hits allowed, Errors, and Walks allowed
Using a manual review, below are the features selected for the second model and the supporting reason/s.

We'll keep the features from the first model (due to low p-values) and add two more features...
TEAM_FIELDING_E = Errors: errors are costly in terms of immediate impact, but could also impact the team in other ways (i.e. a high occurrence could impact team comraderie and confidence in each other)

TEAM_PITCHING_BB = Walks allowed: putting players on base for "free" is more opportunity for points 

<B> Create the Regression Model </B>  
```{r, echo=FALSE}

# Build the second model and produce a summary
second_model <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_PITCHING_H + TEAM_FIELDING_E + TEAM_PITCHING_BB, data = train)
summary(second_model)

```

```{r, echo=FALSE}
#Predict with the second model training data
second_model_predictions = predict(second_model,test)
```

Evaluate the second model results using RMSE
```{r, echo=FALSE}
#Evaluate the second model results using RMSE
rmse(test$TARGET_WINS, second_model_predictions)
```
The increase from two features in the first model to four features in the second model did not yield a noticeable improvement.  The Adjusted R2 on the training data improved slightly, but the RMSE for all practical purposes stayed the same at around 13; which is a poor RMSE implying that both models have poor predictive capability.

## Model #3
### BSR Model (SaberMetrics) (data imputation)
*Base runs (BsR) is a baseball statistic invented by sabermetrician David Smyth to estimate the number of runs a team "should have"*
*scored given their component offensive statistics, as well as the number of runs a hitter or pitcher creates or allows.* 
*It measures essentially the same thing as Bill James runs created, but as sabermetrician Tom M. Tango points out, base* 
*runs models the reality of the run-scoring process "significantly better than any other run estimator".*

*Cleaning Data*
```{r, echo=FALSE}
# load data
data <- read.csv('hw1_train_data.csv')

#imput data by regression: 
data_imp <- mice(data, method = "norm.predict", m = 1)

#complete data 
data_complete <- complete(data_imp)
```

The simplest, uses only the most common batting statistics[2]

$A = H + BB - HR$
$B = (1.4 * TB - .6 * H - 3 * HR + .1 * BB) * 1.02$
$C = AB - H$
$D = HR$

$BsR = \frac{(A * B)}{(B + C)} + D$

```{r, echo=FALSE}
data3 <- data_complete %>% 
  rowwise() %>%
  mutate(TEAM_BATTING_AB = sum( TEAM_BATTING_H,TEAM_BATTING_BB,TEAM_BATTING_SO, na.rm=TRUE),
         TEAM_BATTING_1B = TEAM_BATTING_H - (TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR),
         TEAM_BATTING_TB = TEAM_BATTING_1B + (2 * TEAM_BATTING_2B) + (3 * TEAM_BATTING_3B) + (4 * TEAM_BATTING_HR),
         BSR_A = TEAM_BATTING_H + TEAM_BATTING_BB - TEAM_BATTING_HR,
         BSR_B = (( 1.4 * TEAM_BATTING_TB) - ( 0.6 * TEAM_BATTING_H) - (3 * TEAM_BATTING_HR) + (0.1 * TEAM_BATTING_BB)) * 1.02,
         BSR_C = TEAM_BATTING_AB - TEAM_BATTING_H,
         BSR = ((BSR_A*BSR_B)/(BSR_B + BSR_C)) + TEAM_BATTING_HR
         )

data3 <- as.data.frame(data3)
train3 <- data3 %>% dplyr::sample_frac(.75)
test3  <- dplyr::anti_join(data3, train3, by = 'X')
```

<B> Create the Regression Model </B>  
*BSR*
```{r, echo=FALSE}
rmdata3 <- train3 %>%
  dplyr::select(BSR, TEAM_PITCHING_SO, TEAM_FIELDING_E, TEAM_FIELDING_DP, TARGET_WINS)

#Build the second model and produce a summary
GModel3 <- lm(TARGET_WINS ~ BSR + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP, data = rmdata3)
summary(GModel3)

```

Evaluate the model results using RMSE
```{r, echo=FALSE}

#Predict with the second model training data
GModel3_predictions = predict(GModel3,test3)

#Evaluate the second model results using RMSE
rmse(test3$TARGET_WINS, GModel3_predictions)

```


## Model #4
### (Modified) Backward Elimination Model (omitting NAs)

Due to previously learning how to perform Backward Elimination and it being possible to perform manually, we decided to include a model that resulted from the procedure. The process was performed with imputed data (via MICE) as well as data with NAs removed. The latter showed stronger results, therefore the final model was fitted with the NA omitted data. 

According to Faraway, Backward Elimination is when you start with all predictors in the model, then remove the predictor with the highest p-value as long as it is above your p-value threshold (e.g. 0.05). Then refit the model and continue the process until only predictors with p-values below your threshold remain.

Additionally, we took steps to remove variables with non-intuitive coefficients. For instance, TEAM_FIELDING_DP and TEAM_PITCHING_SO were unexpectedly showing negative effects on wins. While there could be potential intervening variables giving these variables true predictive power, we opted to remove the variables from the model due to the possibility they were significant by chance and due to our bias towards parsimony. Further, RMSE did not drastically worsen when removed.


```{r, echo=FALSE}
# Remove NAs
train_no_na <- na.omit(train) 
test_no_na <- na.omit(test) 
```


```{r, echo=FALSE}

# Fit model
backward_model <- lm(TARGET_WINS ~ TEAM_BASERUN_SB + TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_BASERUN_SB 
                     + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP, data = test_no_na)

# Fit modified model
backward_mod_model <- lm(TARGET_WINS ~ TEAM_BASERUN_SB + TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_FIELDING_E, 
                     data = test_no_na)


# View summary
summary(backward_mod_model)

```


```{r, echo=FALSE}

# Make predictions on test set
backward_model_predictions = predict(backward_mod_model, test_no_na)

# Obtain RMSE between actuals and predicted
rmse(test_no_na$TARGET_WINS, backward_model_predictions)

```


```{r, echo=FALSE}

# Make predictions on evaluation data
backward_model_predictions_evaluation = predict(backward_mod_model, evaluation)

# Final predictions on evaluation set
write.csv(backward_model_predictions_evaluation, 'evaluation_predictions.csv')

```


## SELECT MODELS


### Verifying OLS Regression Assumptions

```{r eval=FALSE, echo=FALSE}
# Assumption: No Multicollinearity (VIF under 5)
vif(backward_mod_model)
```

Assumption: Mean of residuals is zero
```{r, echo=FALSE}
# Assumption: Mean of residuals is zero
mean(residuals(backward_mod_model))
```

```{r, echo=FALSE}
# Assumption: Homoscedasticity of residuals
plot(backward_mod_model)
```

```{r, echo=FALSE}
# Assumption: No auto-correlation
acf(residuals(backward_mod_model), lags=20)
```


### Model Selection

First, before fully evaluating models we validated that all individual predictors had p-values below 0.05, the cutoff for a 95% confidence level. Additionally, we validated that the models F-statistics were also significant at a 95% confidence level.

Then, the two primary statistics used to choose our final model were adjusted R-squared and root mean square error (RMSE). Adjusted R-squared helped guide model selection since, like R-squared, adjusted R-squared measures the amount of variation in the dependent variable explained by the independent variables, except with a correction to ensure only independent variables with predictive power raise the statistic. RMSE was perhaps even more crucial to model selection as it is the measure of the standard deviation of the residuals, essentially a measure of accuracy in the same units as the response variable. To ensure the model can generalize to unobserved data, we calculated the RMSE on our test set. 

Backward elimination saw a RMSE of approximately 10, noticeably outperforming other models. Therefore, we chose the backward elimination model even with a slightly worse adjusted R-squared. Additionally, since all top performing models included four predictors, parsimony was not a consideration.

Lastly, we verified the forward selection model meets OLS regression assumptions. These included: no significant multicollinearity, the mean of residuals is zero, homoscedasticity of residuals, and no significant auto-correlation. We deemed all assumptions had been met, but note, there is a slight trend in the residuals vs fitted plot (Assumption: Homoscedasticity of residuals) which may indicate a small nonlinear trend.


### Matt's References

Bhandari, Aniruddha, "Key Difference between R-squared and Adjusted R-squared for Regression Analysis", Analytics Vidhya, 2020 https://www.analyticsvidhya.com/blog/2020/07/difference-between-r-squared-and-adjusted-r-squared/ 

Glen., Stephanie "RMSE: Root Mean Square Error", StatisticsHowTo.com https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/

Gupta, Aryansh, "Linear Regression Assumptions and Diagnostics in R", RPubs, https://rpubs.com/aryn999/LinearRegressionAssumptionsAndDiagnosticsInR 

Kim, Bommae, "Understanding Diagnostic Plots for Linear Regression Analysis", University of Virginia Library, https://data.library.virginia.edu/diagnostic-plots/ 




## Code Appendix

```{r eval=FALSE, include=TRUE}
## DATA EXPLORATION:
#We can observe the response variable (TARGET_WINS) looks to be normally distributed. This supports the working theory that there are good teams and bad teams. There are also a lot of average teams.

#There are also quite a few variables with missing values. and,Some variables are right skewed (TEAM_BASERUN_CS, TEAM_BASERUN_SB, etc.). This might support the good team theory. It may also introduce non-normally distributed residuals in the model. We shall see.

### Load the Data


# Set seed for reproducibility
set.seed(621)

train <-read.csv("https://raw.githubusercontent.com/akarimhammoud/Data_621/main/Assignment_1/data/moneyball-training-data.csv")

evaluation <-read.csv("https://raw.githubusercontent.com/akarimhammoud/Data_621/main/Assignment_1/data/moneyball-evaluation-data.csv")


# Summary of the data

summary(train)
summary(evaluation)


# Glimpse of the data

glimpse(train)

glimpse(evaluation)


# Find SD for all of the train and test data

apply(train,2,sd, na.rm=TRUE)

apply(evaluation,2,sd, na.rm=TRUE)


# Box plot the data 

ggplot(stack(train), aes(x = ind, y = values)) + 
  geom_boxplot() +
  theme(legend.position="none") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) 

# Variable Distributions

train %>%
  gather(variable, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "#3A8B63", color="#3A8B63") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())


#Log Variable Distributions


train_log <- log(train)

train_log %>%
  gather(variable, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "#3A8B63", color="#3A8B63") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())


# Correlations with Response Variable

train %>%
  gather(variable, value, -TARGET_WINS) %>%
  ggplot(., aes(value, TARGET_WINS)) + 
  geom_point(fill = "#628B3A", color="#628B3A") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")



train %>% 
  cor(., use = "complete.obs") %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", diag = FALSE)



### DATA PREPARATION

# ^[https://statisticsglobe.com/count-number-of-na-values-in-vector-and-column-in-r]

#NA counts for the train data set
colSums(is.na(train))

# ^[https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html]

#visulaization and percentage of NA values
vis_miss(train)



# ^[https://datavizpyr.com/visualizing-missing-data-with-barplot-in-r/]

#alternative NA values visualization
train  %>%
  summarise_all(list(~is.na(.)))%>%
  pivot_longer(everything(),
               names_to = "variables", values_to="missing") %>%
  count(variables, missing) %>%
  ggplot(aes(y=variables,x=n,fill=missing))+
  geom_col()


#Since 92% of the data for the TEAM_BATTING_HBP is missing, the variable has been removed from both test #and train data. TEAM_BASERUN_CS is a runner up with the next highest amount of NA at 34%.


#removes the TEAM_BATTING_HBP due to high # of NAs
train_full <- train %>% dplyr::select(-c(TEAM_BATTING_HBP))
evaluation <- evaluation %>% dplyr::select(-c(TEAM_BATTING_HBP))


# ^[https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R-Manual/R-Manual5.html]

#creates CSV in your current working directory of R
write.csv(train_full,'hw1_train_data.csv')
write.csv(evaluation, 'hw1_evaluation_data.csv')



# Create train, test split
train <- train_full %>% dplyr::sample_frac(.75)
test  <- dplyr::anti_join(train_full, train, by = 'INDEX')



## BUILD MODELS


## Model #1
### Two predictors: Base hits by batters and Hits allowed
#Using a manual review, below are the features selected for the first model and the supporting reason/s.

#TEAM_BATTING_H = Base hits by batters:  it's impossible to win in baseball without getting to the bases # and hitting the ball is the primary means to accomplish this.

#TEAM_PITCHING_H = Hits allowed: winning without a good defense is difficult and in baseball preventing #the other team from getting hits is a good defense strategy.

#Only two features are selected for the first model - start small and build up seems like a good approach.

#<B> Create the Regression Model </B>  

# Build the first model and produce a summary
first_model <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_PITCHING_H, data = train)
summary(first_model)



#The p values are 0, which per the criteria of "keep a feature if the p-value is <0.05" recommends that #we keep both these features.  But, the adjusted R-squared is TERRIBLE at around 21%.  Even though the #R-squared is poor it's simple to run this model with the test data, so we'll do that next. 


#Predict with the first model training data
first_model_predictions = predict(first_model, test)

#Evaluate the first model results using RMSE
rmse(test$TARGET_WINS, first_model_predictions)



## Model #2
### Four predictors: Base hits by batters, Hits allowed, Errors, and Walks allowed
#Using a manual review, below are the features selected for the second model and the supporting reason/s.

#We'll keep the features from the first model (due to low p-values) and add two more features...
#TEAM_FIELDING_E = Errors: errors are costly in terms of immediate impact, but could also impact the team in other ways (i.e. a high occurrence could impact team comraderie and confidence in each other)

#TEAM_PITCHING_BB = Walks allowed: putting players on base for "free" is more opportunity for points 

#<B> Create the Regression Model </B>  


# Build the second model and produce a summary
second_model <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_PITCHING_H + TEAM_FIELDING_E + TEAM_PITCHING_BB, data = train)
summary(second_model)


#Predict with the second model training data
second_model_predictions = predict(second_model,test)

#Evaluate the second model results using RMSE
rmse(test$TARGET_WINS, second_model_predictions)

#The increase from two features in the first model to four features in the second model did not yield a noticeable improvement.  The Adjusted R2 on the training data improved slightly, but the RMSE for all practical purposes stayed the same at around 13; which is a poor RMSE implying that both models have poor predictive capability.

## Model #3
### BSR Model (SaberMetrics) (data imputation)
# *Base runs (BsR) is a baseball statistic invented by sabermetrician David Smyth to estimate the number of runs a team "should have"*
#*scored given their component offensive statistics, as well as the number of runs a hitter or pitcher #creates or allows.* 
#*It measures essentially the same thing as Bill James runs created, but as sabermetrician Tom M. Tango points out, base* 
#*runs models the reality of the run-scoring process "significantly better than any other run estimator".*

#*Cleaning Data*

# load data
data <- read.csv('hw1_train_data.csv')

#imput data by regression: 
data_imp <- mice(data, method = "norm.predict", m = 1)

#complete data 
data_complete <- complete(data_imp)


# The simplest, uses only the most common batting statistics[2]

#$A = H + BB - HR$
#$B = (1.4 * TB - .6 * H - 3 * HR + .1 * BB) * 1.02$
#$C = AB - H$
#$D = HR$

#$BsR = \frac{(A * B)}{(B + C)} + D$


data3 <- data_complete %>% 
  rowwise() %>%
  mutate(TEAM_BATTING_AB = sum( TEAM_BATTING_H,TEAM_BATTING_BB,TEAM_BATTING_SO, na.rm=TRUE),
         TEAM_BATTING_1B = TEAM_BATTING_H - (TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR),
         TEAM_BATTING_TB = TEAM_BATTING_1B + (2 * TEAM_BATTING_2B) + (3 * TEAM_BATTING_3B) + (4 * TEAM_BATTING_HR),
         BSR_A = TEAM_BATTING_H + TEAM_BATTING_BB - TEAM_BATTING_HR,
         BSR_B = (( 1.4 * TEAM_BATTING_TB) - ( 0.6 * TEAM_BATTING_H) - (3 * TEAM_BATTING_HR) + (0.1 * TEAM_BATTING_BB)) * 1.02,
         BSR_C = TEAM_BATTING_AB - TEAM_BATTING_H,
         BSR = ((BSR_A*BSR_B)/(BSR_B + BSR_C)) + TEAM_BATTING_HR
         )

data3 <- as.data.frame(data3)
train3 <- data3 %>% dplyr::sample_frac(.75)
test3  <- dplyr::anti_join(data3, train3, by = 'X')


#<B> Create the Regression Model </B>  
#*BSR*

rmdata3 <- train3 %>%
  dplyr::select(BSR, TEAM_PITCHING_SO, TEAM_FIELDING_E, TEAM_FIELDING_DP, TARGET_WINS)

#Build the second model and produce a summary
GModel3 <- lm(TARGET_WINS ~ BSR + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP, data = rmdata3)
summary(GModel3)



#Predict with the second model training data
GModel3_predictions = predict(GModel3,test3)

#Evaluate the second model results using RMSE
rmse(test3$TARGET_WINS, GModel3_predictions)




## Model #4
### (Modified) Backward Elimination Model (omitting NAs)

#Due to previously learning how to perform Backward Elimination and it being possible to perform manually, we decided to include a model that resulted from the procedure. The process was performed with imputed data (via MICE) as well as data with NAs removed. The latter showed stronger results, therefore the final model was fitted with the NA omitted data. 

#According to Faraway, Backward Elimination is when you start with all predictors in the model, then remove the predictor with the highest p-value as long as it is above your p-value threshold (e.g. 0.05). Then refit the model and continue the process until only predictors with p-values below your threshold remain.

#Additionally, we took steps to remove variables with non-intuitive coefficients. For instance, TEAM_FIELDING_DP and TEAM_PITCHING_SO were unexpectedly showing negative effects on wins. While there could be potential intervening variables giving these variables true predictive power, we opted to remove the variables from the model due to the possibility they were significant by chance and due to our bias towards parsimony. Further, RMSE did not drastically worsen when removed.


# Remove NAs
train_no_na <- na.omit(train) 
test_no_na <- na.omit(test) 

# Fit model
backward_model <- lm(TARGET_WINS ~ TEAM_BASERUN_SB + TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_BASERUN_SB 
                     + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP, data = test_no_na)

# Fit modified model
backward_mod_model <- lm(TARGET_WINS ~ TEAM_BASERUN_SB + TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_FIELDING_E, 
                     data = test_no_na)


# View summary
summary(backward_mod_model)



# Make predictions on test set
backward_model_predictions = predict(backward_mod_model, test_no_na)

# Obtain RMSE between actuals and predicted
rmse(test_no_na$TARGET_WINS, backward_model_predictions)



# Make predictions on evaluation data
backward_model_predictions_evaluation = predict(backward_mod_model, evaluation)

# Final predictions on evaluation set
write.csv(backward_model_predictions_evaluation, 'evaluation_predictions.csv')




## SELECT MODELS


### Verifying OLS Regression Assumptions

# Assumption: No Multicollinearity (VIF under 5)
vif(backward_mod_model)

# Assumption: Mean of residuals is zero
mean(residuals(backward_mod_model))

# Assumption: Homoscedasticity of residuals
plot(backward_mod_model)

# Assumption: No auto-correlation
acf(residuals(backward_mod_model), lags=20)


```

