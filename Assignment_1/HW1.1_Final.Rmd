---
title: "Data 621 Assignment 1"
subtitle: ""
author: "Group2 - Gabriella Martinez, Maliat Islam, Ken Popkin, George Cruz Deschamps, Matthew Lucich and Karim Hammoud"
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: pygments
    theme: cerulean 
    toc: yes
    toc_float: yes
  pdf_document: default
  openintro::lab_report: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```


In this data set we are trying to identify good and bad teams in major league baseball teamâ€™s season. We are assuming some of the predictors will be higher for good teams. We will try to predict how many times a team will win in this season.

```{r, include = FALSE}
library(tidyverse)
library(corrplot)
library(pacman)
library(here)
library(RCurl)
library(tidyverse)
library(naniar)
library(UpSetR)
library(mice)
library(dplyr)
library(Metrics)
library(MLmetrics)
```


## DATA EXPLORATION:
We can observe the response variable (TARGET_WINS) looks to be normally distributed. This supports the working theory that there are good teams and bad teams. There are also a lot of average teams.

There are also quite a few variables with missing values. and,Some variables are right skewed (TEAM_BASERUN_CS, TEAM_BASERUN_SB, etc.). This might support the good team theory. It may also introduce non-normally distributed residuals in the model. We shall see.

### Load the Data

```{r, echo=FALSE, results='asis', cache=TRUE}
train <-read.csv("https://raw.githubusercontent.com/akarimhammoud/Data_621/main/Assignment_1/data/moneyball-training-data.csv")

test <-read.csv("https://raw.githubusercontent.com/akarimhammoud/Data_621/main/Assignment_1/data/moneyball-evaluation-data.csv")
```

Summary of the data
```{r}
summary(train)
summary(test)
```

Glimpse of the data
```{r}
glimpse(train)

glimpse(test)
```


```{r warning=FALSE}
train %>%
  gather(variable, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "#3A8B63", color="#3A8B63") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())
```


```{r}
temp <- train %>% 
  cor(., use = "complete.obs") #%>%
  
temp[lower.tri(temp, diag=TRUE)] <- ""
temp <- temp %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(Variable, Correlation, -rowname) %>%
  filter(Variable != rowname) %>%
  filter(Correlation != "") %>%
  mutate(Correlation = as.numeric(Correlation)) %>%
  rename(` Variable` = rowname) %>%
  arrange(desc(abs(Correlation))) 
```

### Correlations with Response Variable
```{r warning=FALSE}
train %>%
  gather(variable, value, -TARGET_WINS) %>%
  ggplot(., aes(value, TARGET_WINS)) + 
  geom_point(fill = "#628B3A", color="#628B3A") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")
```

```{r}
train %>% 
  cor(., use = "complete.obs") %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", diag = FALSE)
```


### DATA PREPARATION


```{r}
#remove index column from both test and train data
train <- train %>% 
  select(2:17)
test <- test %>% 
  select(2:16)
```


```colSums```^[https://statisticsglobe.com/count-number-of-na-values-in-vector-and-column-in-r]

```{r}
#NA counts for the train data set
colSums(is.na(train))
```

^[https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html]
```{r}
#visulaization and percentage of NA values
vis_miss(train)
```


^[https://datavizpyr.com/visualizing-missing-data-with-barplot-in-r/]
```{r}
#alternative NA values visualization
train  %>%
  summarise_all(list(~is.na(.)))%>%
  pivot_longer(everything(),
               names_to = "variables", values_to="missing") %>%
  count(variables, missing) %>%
  ggplot(aes(y=variables,x=n,fill=missing))+
  geom_col()
```

Since 92% of the data for the TEAM_BATTING_HBP is missing, the variable has been removed from both test and train data. TEAM_BASERUN_CS is a runner up with the next highest amount of NA at 34%.

```{r}
#removes the TEAM_BATTING_HBP due to high # of NAs
train <- train %>% 
  select(-c(TEAM_BATTING_HBP))
test <- test %>% 
  select(-c(TEAM_BATTING_HBP))
```

^[https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R-Manual/R-Manual5.html]
```{r}
#creates CSV in your current working directory of R
write.csv(train, 'hw1_train_data.csv')
write.csv(test, 'hw1_test_data.csv')
```

^[https://stats.idre.ucla.edu/r/faq/how-do-i-perform-multiple-imputation-using-predictive-mean-matching-in-r/]
```{r}
#build_kmeans()

imputed_train_data <- mice(train, m=1)

imputed_test_data <- mice(test, m=1)
```

```{r}
#appends the imputed data to the original data
imp_train <- complete(imputed_train_data, "long", inc = TRUE)

imp_test <- complete(imputed_test_data, "long", inc = TRUE)
```

```{r}
#Imputation Diagnostic Checks

## labels observed data in blue and imputed data in red for y1
col <- rep(c("blue", "red")[1 + as.numeric(is.na(imputed_train_data$data$TEAM_BATTING_H))], 6)
## plots data for y1 by imputation
stripplot(TEAM_BATTING_H ~ .imp, data = imp_train, jit = TRUE, col = col, xlab = "imputation Number")
```



## BUILD MODELS

```{r echo=FALSE}
# load data
data <- read.csv('https://raw.githubusercontent.com/akarimhammoud/Data_621/main/Assignment_1/data/moneyball-training-data.csv')

cases = dim(data)[1]
features = dim(data)[2]
cat('Data for this project is', cases, 'cases and', features, 'features')
```

```{r data}
head(data,1)
```

### Create train and test data
```{r}
data$id <- 1:nrow(data)
train <- data %>% dplyr::sample_frac(.75)
test  <- dplyr::anti_join(data, train, by = 'INDEX')
```

```{r}
cases = dim(train)[1]
features = dim(train)[2]
cat('Training data for this project is', cases, 'cases and', features, 'features')
```

```{r}
cases = dim(test)[1]
features = dim(test)[2]
cat('Testing data for this project is', cases, 'cases and', features, 'features')
```

### First Model
Using a manual review, below are the features selected for the first model and the supporting reason/s.

TEAM_BATTING_H = Base hits by batters:  it's impossible to win in baseball without getting to the bases and hitting the ball is the primary means to accomplish this.

TEAM_PITCHING_H = Hits allowed: winning without a good defense is difficult and in baseball preventing the other team from getting hits is a good defense strategy.

Only two features are selected for the first model - start small and build up seems like a good approach.

<B> Create the Regression Model </B>  
```{r}
#Select the desired data for the model
rmdata <- train %>%
  select(TEAM_BATTING_H, TEAM_PITCHING_H, TARGET_WINS)

#Build the first model and produce a summary
first_model <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_PITCHING_H, data = rmdata)
summary(first_model)
```

The p values are 0, which per the criteria of "keep a feature if the p-value is <0.05" recommends that we keep both these features.  But, the adjusted R-squared is TERRIBLE at around 21%.  Even though the R-squared is poor it's simple to run this model with the test data, so we'll do that next. 

```{r}
#Predict with the first model training data
first_model_predictions = predict(first_model,test)
```

```{r}
#Evaluate the first model results using RMSE
rmse(test$TARGET_WINS, first_model_predictions)
```

### Second Model
Using a manual review, below are the features selected for the second model and the supporting reason/s.

We'll keep the features from the first model (due to low p-values) and add two more features...
TEAM_FIELDING_E = Errors: errors are costly in terms of immediate impact, but could also impact the team in other ways (i.e. a high occurrence could impact team comraderie and confidence in each other)

TEAM_PITCHING_BB = Walks allowed: putting players on base for "free" is more opportunity for points 

<B> Create the Regression Model </B>  
```{r}
#Select the desired data for the model
rmdata <- train %>%
  select(TEAM_BATTING_H, TEAM_PITCHING_H, TEAM_FIELDING_E, TEAM_PITCHING_BB, TARGET_WINS)

#Build the second model and produce a summary
second_model <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_PITCHING_H + TEAM_FIELDING_E + TEAM_PITCHING_BB, data = rmdata)
summary(second_model)
```

```{r}
#Predict with the second model training data
second_model_predictions = predict(second_model,test)
```

```{r}
#Evaluate the second model results using RMSE
rmse(test$TARGET_WINS, second_model_predictions)
```
The increase from two features in the first model to four features in the second model did not yield a noticeable improvement.  The Adjusted R2 on the training data improved slightly, but the RMSE for all practical purposes stayed the same at around 13; which is a poor RMSE implying that both models have poor predictive capability.

### Third Model




## SELECT MODELS

